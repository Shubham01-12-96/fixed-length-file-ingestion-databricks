{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84339ae9-fd96-4800-811c-255aee7ca2af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Fixed-Length File Processing using Databricks & PySpark\n",
    "\n",
    "This notebook demonstrates how to ingest and process fixed-length files using\n",
    "PySpark on Databricks. The solution reflects a real-world data engineering use\n",
    "case where legacy systems generate fixed-width files that must be transformed\n",
    "into structured datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95b7c586-d736-4f16-be76-fe2bba4fa522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Notebook Workflow\n",
    "\n",
    "This notebook performs the following steps:\n",
    "1. Read raw fixed-length file from Databricks File System (DBFS)\n",
    "2. Define column positions and schema\n",
    "3. Parse raw records into structured format\n",
    "4. Apply data cleansing and transformations\n",
    "5. Validate and display the final output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d836246e-e430-426e-b43f-41666cfbbd24",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "List of Libraries"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84835e74-103f-4257-800b-2d747f93243d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Input Data Description\n",
    "\n",
    "The input file is a fixed-length text file stored in DBFS.\n",
    "Each record contains multiple fields with predefined start and end positions.\n",
    "Since the file has no delimiters, column boundaries must be explicitly defined\n",
    "before parsing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3af83a5e-74a1-4a71-b38c-b2c428e1f304",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define  Variables"
    }
   },
   "outputs": [],
   "source": [
    "json_path = \"/Volumes/practice/default/practice/fixedlength_json.json\"\n",
    "csv_path = \"/Volumes/practice/default/practice/FixLengthFile.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c424e283-fed2-455d-8856-5f6258d78127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Read Fixed-Length File from DBFS\n",
    "\n",
    "The file is read as raw text because fixed-length files do not contain\n",
    "delimiters. Each row represents a single record that will be parsed\n",
    "based on column positions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efc3d3a3-90f5-4688-b8ae-9f50811d7c14",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reading Files"
    }
   },
   "outputs": [],
   "source": [
    "df_json = spark.read.format(\"json\").option(\"multiline\",True).load(json_path)\n",
    "\n",
    "df_csv = spark.read.format(\"csv\").load(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fd753bf-31ec-4916-9bc6-52883741fd15",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Displaying the Result Set"
    }
   },
   "outputs": [],
   "source": [
    "df_csv.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1440cc66-b61f-4b3b-b8d4-59b15db0177e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Define Column Positions and Schema\n",
    "\n",
    "Column positions are defined based on the fixed-width specification.\n",
    "Each field is extracted using substring logic and mapped to a\n",
    "meaningful column name and data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec0732fb-149d-40ef-a76b-047f2de02479",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Defining Custom Schema for our Data"
    }
   },
   "outputs": [],
   "source": [
    "schema_data = [\n",
    "    {\"name\" : \"pid\", \"Type\":\"String\", \"prelength\": 0, \"length\": 4},\n",
    "    {\"name\" : \"pname\", \"Type\":\"String\", \"prelength\": 4, \"length\": 12},\n",
    "    {\"name\" : \"quantity\", \"Type\":\"String\", \"prelength\": 12, \"length\": 15},\n",
    "    {\"name\" : \"amount\", \"Type\":\"String\", \"prelength\": 18, \"length\": 23}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78c09f52-f823-404a-b8fa-4be18d359155",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Convert DataFrame to RDD for Row-Level Processing\n",
    "\n",
    "At this stage, the DataFrame is converted into an RDD to enable\n",
    "low-level, record-by-record processing.\n",
    "\n",
    "Fixed-length file parsing requires direct access to raw string\n",
    "records so that individual character positions can be extracted\n",
    "accurately. Converting the DataFrame to an RDD provides the\n",
    "flexibility needed for such custom parsing logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78130c63-6b9d-4489-b370-92ccb51ee3b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Transformation Explanation\n",
    "\n",
    "- The DataFrame contains a single column with raw fixed-length records\n",
    "- Calling `.rdd` converts the DataFrame into an RDD of Row objects\n",
    "- The `map(lambda x: x[0])` extracts the raw string value from each Row\n",
    "\n",
    "After this step, the data becomes an RDD where each element represents\n",
    "one complete fixed-length record as a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95f3ca9f-fd55-4d6e-a32f-4cd1c6ce773b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Converting our DataFrame to RDD for Row level control"
    }
   },
   "outputs": [],
   "source": [
    "rdd_lines = df_csv.rdd.map(lambda x : x[0])\n",
    "\n",
    "rdd_lines.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6311daa3-0d05-47df-905d-5c01a131d5df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Enumerate Fixed-Length Schema Metadata\n",
    "\n",
    "This step iterates over the schema definition that describes the\n",
    "fixed-length file structure.\n",
    "\n",
    "Each schema entry contains metadata such as column start position\n",
    "and length. Enumerating the schema allows us to track column order\n",
    "and verify field boundaries before applying extraction logic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91959ced-b6e2-4cde-a292-0e08cb1a8b71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Why Schema Enumeration Is Required\n",
    "\n",
    "Enumerating the schema ensures:\n",
    "- Correct column sequencing during extraction\n",
    "- Validation of start and end positions\n",
    "- Alignment between raw data and target schema\n",
    "\n",
    "This step acts as a safeguard to prevent incorrect slicing of\n",
    "fixed-length records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66829af8-d490-4980-8631-ae0b93dabf4d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Enumerate and Print Schema Data"
    }
   },
   "outputs": [],
   "source": [
    "for i in enumerate(schema_data):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e2f50c2-e30f-45d4-b690-dc995a60af5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Extract Fields from Fixed-Length Records\n",
    "\n",
    "In this step, each raw fixed-length record is parsed into individual\n",
    "fields using schema-defined character positions.\n",
    "\n",
    "A custom mapping function is applied to extract substrings from\n",
    "each record and assemble them into a structured tuple.\n",
    "\n",
    "### Field Extraction Logic Explained\n",
    "\n",
    "- Each RDD element represents a single raw text record\n",
    "- The schema defines `prelength` (start position) and `length` (end position)\n",
    "- Substrings are extracted using character slicing\n",
    "- Extracted values are combined into a tuple representing one structured row\n",
    "\n",
    "The output of this step is an RDD of tuples, where each tuple\n",
    "corresponds to a parsed record aligned with the target schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43433b7f-68f2-41c8-978f-546251ff69e5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Extracting Tuple Data from RDD Using Custom Schema"
    }
   },
   "outputs": [],
   "source": [
    "rdd_final = rdd_lines.map(\n",
    "    lambda p : tuple(p[z[\"prelength\"] : z[\"length\"]]\n",
    "        for i , z in enumerate(schema_data)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a92e31d0-e17b-4298-8257-a163405f5ef8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6: Build Spark Schema from Fixed-Length Metadata\n",
    "\n",
    "In this step, a Spark `StructType` schema is dynamically constructed\n",
    "using the fixed-length file metadata.\n",
    "\n",
    "Each column name defined in the schema configuration is mapped to a\n",
    "Spark `StructField`. This approach ensures that the final DataFrame\n",
    "has well-defined column names and a consistent structure.\n",
    "\n",
    "### Why Schema Construction Is Required\n",
    "\n",
    "After parsing fixed-length records into tuples, the data lacks\n",
    "column names and data types. Defining an explicit schema:\n",
    "\n",
    "- Improves data readability and usability\n",
    "- Enforces structural consistency\n",
    "- Enables downstream validation and transformations\n",
    "- Alignes the output with analytics and reporting requirements\n",
    "\n",
    "### Schema Construction Logic Explained\n",
    "\n",
    "- Iterates over the schema metadata\n",
    "- Creates a `StructField` for each column name\n",
    "- Assigns `StringType` to maintain flexibility during ingestion\n",
    "- Combines all fields into a `StructType` schema\n",
    "\n",
    "This schema will be applied when converting the RDD into a\n",
    "structured Spark DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f053271-80f3-4c00-8b7e-ea238b5ed9b8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Custom Schema from Schema Data"
    }
   },
   "outputs": [],
   "source": [
    "schema_field = []\n",
    "\n",
    "for i in schema_data:\n",
    "    schema_field.append(StructField(i[\"name\"], StringType(), True))\n",
    "\n",
    "final_schema = StructType(schema_field)\n",
    "print(final_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b6ccf4e-3681-4c4f-ba35-87125d9ebf47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 7: Convert Parsed RDD to Structured DataFrame\n",
    "\n",
    "The parsed RDD of tuples is converted into a Spark DataFrame by\n",
    "applying the previously defined schema.\n",
    "\n",
    "This transformation transitions the data from low-level RDDs to\n",
    "high-level DataFrames, enabling optimized Spark execution and\n",
    "SQL-based analytics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42a8c60f-a167-4e79-a2d4-5c4cf0b25353",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd_final.toDF(schema = final_schema).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be57f51b-0219-4c70-9ec3-2579ad561ce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Final Data Transformation Flow\n",
    "\n",
    "- Input: RDD[Tuple] representing parsed fixed-length records\n",
    "- Schema Applied: StructType with meaningful column names\n",
    "- Output: Spark DataFrame ready for analytics and downstream pipelines\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "fixed-length-file-ingestion-databricks-pyspark.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}